{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"hMPkG1yKz7AX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656880541948,"user_tz":240,"elapsed":32025,"user":{"displayName":"Brayden Goldstein-Gelb","userId":"13918462711668364282"}},"outputId":"b9b81282-7f7b-4323-e83a-6a8330b0e5c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Spring22/decoupled-style-descriptors-eb\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting virtualenv\n","  Downloading virtualenv-20.15.1-py2.py3-none-any.whl (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 26.0 MB/s \n","\u001b[?25hCollecting distlib<1,>=0.3.1\n","  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n","\u001b[K     |████████████████████████████████| 461 kB 17.6 MB/s \n","\u001b[?25hRequirement already satisfied: six<2,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (1.15.0)\n","Collecting platformdirs<3,>=2\n","  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n","Requirement already satisfied: filelock<4,>=3.2 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (3.7.1)\n","Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->virtualenv) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->virtualenv) (4.1.1)\n","Installing collected packages: platformdirs, distlib, virtualenv\n","Successfully installed distlib-0.3.4 platformdirs-2.5.2 virtualenv-20.15.1\n","created virtual environment CPython3.7.13.final.0-64 in 19074ms\n","  creator CPython3Posix(dest=/content/drive/MyDrive/Colab Notebooks/Spring22/decoupled-style-descriptors-eb/venv, clear=False, no_vcs_ignore=False, global=False)\n","  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n","    added seed packages: pip==22.1.2, setuptools==62.6.0, wheel==0.37.1\n","  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 28.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n","Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.5.1\n"]}],"source":["# might be different bepending on your file structure in drive\n","%cd \"/content/drive/MyDrive/Colab Notebooks/Spring22/decoupled-style-descriptors-eb\"\n","!pip install virtualenv\n","!virtualenv venv\n","!source ./venv/bin/activate\n","!pip install tensorboardX"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Nn6_7M8v5fXW","executionInfo":{"status":"ok","timestamp":1656880561271,"user_tz":240,"elapsed":5624,"user":{"displayName":"Brayden Goldstein-Gelb","userId":"13918462711668364282"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from tensorboardX import SummaryWriter\n","from PIL import Image, ImageDraw, ImageFont\n","from DataLoader import DataLoader\n","import pickle\n","from config.GlobalVariables import *\n","import os\n","import argparse\n","from SynthesisNetwork import SynthesisNetwork"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"mgWvy08x1Y9l","executionInfo":{"status":"ok","timestamp":1656880563547,"user_tz":240,"elapsed":152,"user":{"displayName":"Brayden Goldstein-Gelb","userId":"13918462711668364282"}}},"outputs":[],"source":["# constants, based on:\n","# \n","divider     = 5.0\n","weight_dim\t= 256\n","num_samples\t= 5\n","did\t\t\t    = 0\n","num_layers\t= 3\n","num_writer\t= 1\n","lr\t\t\t    = 0.01\n","no_char\t\t  = 0\n","VALIDATION  = True\n","\n","sentence_loss = True\n","word_loss = True\n","segment_loss = True\n","TYPE_A = True\n","TYPE_B = False\n","TYPE_C = False\n","TYPE_D = False\n","REC = False\n","ORIGINAL = False\n","LOAD_FROM_CHECKPOINT = True # Set to false for training from sratch, true to use latest checkpoint"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"4sNGEozt7iYq","executionInfo":{"status":"ok","timestamp":1656880567315,"user_tz":240,"elapsed":153,"user":{"displayName":"Brayden Goldstein-Gelb","userId":"13918462711668364282"}}},"outputs":[],"source":["cwds = os.getcwd()\n","cwd = cwds.split('/')[-1]\n","datadir = './data/writers'"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"FI0w4Pyx729j","executionInfo":{"status":"ok","timestamp":1656880575206,"user_tz":240,"elapsed":6555,"user":{"displayName":"Brayden Goldstein-Gelb","userId":"13918462711668364282"}}},"outputs":[],"source":["if sentence_loss:\n","  writer_sentence = SummaryWriter(logdir='./runs/sentence-' + cwd)\n","  if VALIDATION:\n","    valid_writer_sentence = SummaryWriter(logdir='./runs/valid-sentence-' + cwd)\n","\n","if word_loss:\n","  writer_word = SummaryWriter(logdir='./runs/word-' + cwd)\n","  if VALIDATION:\n","    valid_writer_word = SummaryWriter(logdir='./runs/valid-word-' + cwd)\n","\n","if segment_loss:\n","  writer_segment = SummaryWriter(logdir='./runs/segment-' + cwd)\n","  if VALIDATION:\n","    valid_writer_segment = SummaryWriter(logdir='./runs/valid-segment-' + cwd)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"n70ge5u_9_yl","executionInfo":{"status":"ok","timestamp":1656880577240,"user_tz":240,"elapsed":168,"user":{"displayName":"Brayden Goldstein-Gelb","userId":"13918462711668364282"}}},"outputs":[],"source":["timestep\t\t= 0\n","grad_clip\t\t= 10.0\n","device\t\t\t= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","if device == \"cuda\":\n","  torch.cuda.set_device(did)\n","else:\n","  num_writer\t\t= 1\n","  num_samples\t\t= 3"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"qXE8ZGqR-EPP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656880582440,"user_tz":240,"elapsed":3401,"user":{"displayName":"Brayden Goldstein-Gelb","userId":"13918462711668364282"}},"outputId":"a94476f4-d0bf-4343-acce-dd505a0ecf2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["True True True\n","self.datadir :  ./data/writers\n"]}],"source":["writer_all\t\t= SummaryWriter(logdir='./runs/all-'+cwd)\n","if VALIDATION:\n","  valid_writer_all\t= SummaryWriter(logdir='./runs/valid-all-'+cwd)\n","\n","print (sentence_loss, word_loss, segment_loss)\n","net\t= SynthesisNetwork(weight_dim=weight_dim, num_layers=num_layers, sentence_loss=sentence_loss, word_loss=word_loss, segment_loss=segment_loss, TYPE_A=TYPE_A, TYPE_B=TYPE_B, TYPE_C=TYPE_C, TYPE_D=TYPE_D, ORIGINAL=ORIGINAL, REC=REC)\n","_\t= net.to(device)\n","\n","for param in net.parameters():\n","  nn.init.normal_(param, mean=0.0, std=0.075)\n","\n","dl\t\t\t\t= DataLoader(num_writer=num_writer, num_samples=num_samples, divider=divider, datadir=datadir)\n","\n","optimizer\t\t= optim.Adam(net.parameters(), lr=lr)\n","step_size\t\t= int(10000 / (num_writer * num_samples))\n","scheduler \t\t= optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.99)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"HPHG8IoCgEK4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656880588534,"user_tz":240,"elapsed":5001,"user":{"displayName":"Brayden Goldstein-Gelb","userId":"13918462711668364282"}},"outputId":"d7269d7c-8e8d-42e8-b874-40b747af451a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded from checkpoint 130000\n"]}],"source":["if LOAD_FROM_CHECKPOINT:\n","  checkpoints = os.listdir(\"./model\")\n","  max_timestep = max([int(filename[:-3]) for filename in checkpoints if \".pt\" in filename])\n","  checkpoint = torch.load(f\"./model/{max_timestep}.pt\", map_location=torch.device('cpu'))\n","  net.load_state_dict(checkpoint['model_state_dict'])\n","  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","  timestep = checkpoint['timestep']\n","  print(f\"Loaded from checkpoint {timestep}\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":746},"id":"7V7n9jHA9ozw","outputId":"0c3d0a0d-850a-46d1-af21-988cef134ca8","executionInfo":{"status":"error","timestamp":1656880641190,"user_tz":240,"elapsed":47408,"user":{"displayName":"Brayden Goldstein-Gelb","userId":"13918462711668364282"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n","NaN or Inf found in input tensor.\n"]},{"output_type":"stream","name":"stdout","text":["Step : 130003 \tLoss : nan \tlr : 0.001\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-038bd6839ef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mbatch_word_level_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_word_level_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_word_level_char_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_segment_level_stroke_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mbatch_segment_level_stroke_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_segment_level_stroke_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_segment_level_term\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         batch_segment_level_char, batch_segment_level_char_length])\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/Spring22/decoupled-style-descriptors-eb/SynthesisNetwork.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    447\u001b[0m                                                 \u001b[0munique_char_out_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_lstm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_char_input_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                                                 \u001b[0munique_char_out_1\u001b[0m                       \u001b[0;34m=\u001b[0m \u001b[0munique_char_out_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                                                 \u001b[0munique_char_out_1\u001b[0m                       \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_vec_fc2_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_char_out_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                                                 \u001b[0munique_char_matrix_1\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0munique_char_out_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                                                 \u001b[0munique_char_matrix_1\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0munique_char_matrix_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["while True:\n","  optimizer.zero_grad()\n","  timestep\t\t   += num_writer * num_samples\n","\n","  [all_sentence_level_stroke_in, all_sentence_level_stroke_out, all_sentence_level_stroke_length, all_sentence_level_term,\n","  all_sentence_level_char, all_sentence_level_char_length, all_word_level_stroke_in, all_word_level_stroke_out,\n","  all_word_level_stroke_length, all_word_level_term, all_word_level_char, all_word_level_char_length,\n","  all_segment_level_stroke_in, all_segment_level_stroke_out, all_segment_level_stroke_length, all_segment_level_term,\n","  all_segment_level_char, all_segment_level_char_length] = dl.next_batch(TYPE='TRAIN')\n","\n","  batch_sentence_level_stroke_in \t\t= [torch.FloatTensor(a).to(device) for a in all_sentence_level_stroke_in]\n","  batch_sentence_level_stroke_out \t= [torch.FloatTensor(a).to(device) for a in all_sentence_level_stroke_out]\n","  batch_sentence_level_stroke_length \t= [torch.LongTensor(a).to(device).unsqueeze(-1) for a in all_sentence_level_stroke_length]\n","  batch_sentence_level_term \t\t\t= [torch.FloatTensor(a).to(device) for a in all_sentence_level_term]\n","  batch_sentence_level_char \t\t\t= [torch.LongTensor(a).to(device) for a in all_sentence_level_char]\n","  batch_sentence_level_char_length \t= [torch.LongTensor(a).to(device).unsqueeze(-1) for a in all_sentence_level_char_length]\n","  batch_word_level_stroke_in \t\t\t= [torch.FloatTensor(a).to(device) for a in all_word_level_stroke_in]\n","  batch_word_level_stroke_out \t\t= [torch.FloatTensor(a).to(device) for a in all_word_level_stroke_out]\n","  batch_word_level_stroke_length \t\t= [torch.LongTensor(a).to(device).unsqueeze(-1) for a in all_word_level_stroke_length]\n","  batch_word_level_term \t\t\t\t= [torch.FloatTensor(a).to(device) for a in all_word_level_term]\n","  batch_word_level_char \t\t\t\t= [torch.LongTensor(a).to(device) for a in all_word_level_char]\n","  batch_word_level_char_length \t\t= [torch.LongTensor(a).to(device).unsqueeze(-1) for a in all_word_level_char_length]\n","  batch_segment_level_stroke_in \t\t= [[torch.FloatTensor(a).to(device) for a in b] for b in all_segment_level_stroke_in]\n","  batch_segment_level_stroke_out \t\t= [[torch.FloatTensor(a).to(device) for a in b] for b in all_segment_level_stroke_out]\n","  batch_segment_level_stroke_length \t= [[torch.LongTensor(a).to(device).unsqueeze(-1) for a in b] for b in all_segment_level_stroke_length]\n","  batch_segment_level_term \t\t\t= [[torch.FloatTensor(a).to(device) for a in b] for b in all_segment_level_term]\n","  batch_segment_level_char \t\t\t= [[torch.LongTensor(a).to(device) for a in b] for b in all_segment_level_char]\n","  batch_segment_level_char_length \t= [[torch.LongTensor(a).to(device).unsqueeze(-1) for a in b] for b in all_segment_level_char_length]\n","\n","  res = net([batch_sentence_level_stroke_in, batch_sentence_level_stroke_out, batch_sentence_level_stroke_length,\n","      batch_sentence_level_term, batch_sentence_level_char, batch_sentence_level_char_length,\n","      batch_word_level_stroke_in, batch_word_level_stroke_out, batch_word_level_stroke_length,\n","      batch_word_level_term, batch_word_level_char, batch_word_level_char_length, batch_segment_level_stroke_in,\n","      batch_segment_level_stroke_out, batch_segment_level_stroke_length, batch_segment_level_term,\n","      batch_segment_level_char, batch_segment_level_char_length])\n","\n","  total_loss, sentence_losses, word_losses, segment_losses = res\n","\n","  print (\"Step :\", timestep, \"\\tLoss :\", total_loss.item(), \"\\tlr :\", optimizer.param_groups[0]['lr'])\n","\n","  writer_all.add_scalar('ALL/total_loss', total_loss, timestep)\n","\n","  if sentence_loss:\n","    [total_sentence_loss, mean_sentence_W_consistency_loss, mean_ORIGINAL_sentence_termination_loss, mean_ORIGINAL_sentence_loc_reconstruct_loss, mean_ORIGINAL_sentence_touch_reconstruct_loss, mean_TYPE_A_sentence_termination_loss, mean_TYPE_A_sentence_loc_reconstruct_loss, mean_TYPE_A_sentence_touch_reconstruct_loss, mean_TYPE_B_sentence_termination_loss, mean_TYPE_B_sentence_loc_reconstruct_loss, mean_TYPE_B_sentence_touch_reconstruct_loss, mean_TYPE_A_sentence_WC_reconstruct_loss, mean_TYPE_B_sentence_WC_reconstruct_loss] = sentence_losses\n","\n","    writer_all.add_scalar('ALL/total_sentence_loss', total_sentence_loss, timestep)\n","    writer_sentence.add_scalar('Loss/mean_W_consistency_loss', mean_sentence_W_consistency_loss, timestep)\n","    if ORIGINAL:\n","      writer_sentence.add_scalar('Loss/mean_ORIGINAL_loss', mean_ORIGINAL_sentence_termination_loss + mean_ORIGINAL_sentence_loc_reconstruct_loss + mean_ORIGINAL_sentence_touch_reconstruct_loss, timestep)\n","      writer_sentence.add_scalar('Z_LOSS/mean_ORIGINAL_termination_loss', mean_ORIGINAL_sentence_termination_loss, timestep)\n","      writer_sentence.add_scalar('Loss_Loc/mean_ORIGINAL_loc_reconstruct_loss', mean_ORIGINAL_sentence_loc_reconstruct_loss, timestep)\n","      writer_sentence.add_scalar('Z_LOSS/mean_ORIGINAL_touch_reconstruct_loss', mean_ORIGINAL_sentence_touch_reconstruct_loss, timestep)\n","    if TYPE_A:\n","      writer_sentence.add_scalar('Loss/mean_TYPE_A_loss', mean_TYPE_A_sentence_termination_loss + mean_TYPE_A_sentence_loc_reconstruct_loss + mean_TYPE_A_sentence_touch_reconstruct_loss, timestep)\n","      writer_sentence.add_scalar('Z_LOSS/mean_TYPE_A_termination_loss', mean_TYPE_A_sentence_termination_loss, timestep)\n","      writer_sentence.add_scalar('Loss_Loc/mean_TYPE_A_loc_reconstruct_loss', mean_TYPE_A_sentence_loc_reconstruct_loss, timestep)\n","      writer_sentence.add_scalar('Z_LOSS/mean_TYPE_A_touch_reconstruct_loss', mean_TYPE_A_sentence_touch_reconstruct_loss, timestep)\n","      writer_sentence.add_scalar('Z_LOSS/mean_TYPE_A_WC_reconstruct_loss', mean_TYPE_A_sentence_WC_reconstruct_loss, timestep)\n","    if TYPE_B:\n","      writer_sentence.add_scalar('Loss/mean_TYPE_B_loss', mean_TYPE_B_sentence_termination_loss + mean_TYPE_B_sentence_loc_reconstruct_loss + mean_TYPE_B_sentence_touch_reconstruct_loss, timestep)\n","      writer_sentence.add_scalar('Z_LOSS/mean_TYPE_B_termination_loss', mean_TYPE_B_sentence_termination_loss, timestep)\n","      writer_sentence.add_scalar('Loss_Loc/mean_TYPE_B_loc_reconstruct_loss', mean_TYPE_B_sentence_loc_reconstruct_loss, timestep)\n","      writer_sentence.add_scalar('Z_LOSS/mean_TYPE_B_touch_reconstruct_loss', mean_TYPE_B_sentence_touch_reconstruct_loss, timestep)\n","      writer_sentence.add_scalar('Z_LOSS/mean_TYPE_B_WC_reconstruct_loss', mean_TYPE_B_sentence_WC_reconstruct_loss, timestep)\n","\n","  if word_loss:\n","    [total_word_loss, mean_word_W_consistency_loss, mean_ORIGINAL_word_termination_loss, mean_ORIGINAL_word_loc_reconstruct_loss, mean_ORIGINAL_word_touch_reconstruct_loss, mean_TYPE_A_word_termination_loss, mean_TYPE_A_word_loc_reconstruct_loss, mean_TYPE_A_word_touch_reconstruct_loss, mean_TYPE_B_word_termination_loss, mean_TYPE_B_word_loc_reconstruct_loss, mean_TYPE_B_word_touch_reconstruct_loss, mean_TYPE_C_word_termination_loss, mean_TYPE_C_word_loc_reconstruct_loss, mean_TYPE_C_word_touch_reconstruct_loss, mean_TYPE_D_word_termination_loss, mean_TYPE_D_word_loc_reconstruct_loss, mean_TYPE_D_word_touch_reconstruct_loss, mean_TYPE_A_word_WC_reconstruct_loss, mean_TYPE_B_word_WC_reconstruct_loss, mean_TYPE_C_word_WC_reconstruct_loss, mean_TYPE_D_word_WC_reconstruct_loss] = word_losses\n","    writer_all.add_scalar('ALL/total_word_loss', total_word_loss, timestep)\n","    writer_word.add_scalar('Loss/mean_W_consistency_loss', mean_word_W_consistency_loss, timestep)\n","\n","    if ORIGINAL:\n","      writer_word.add_scalar('Loss/mean_ORIGINAL_loss', mean_ORIGINAL_word_termination_loss + mean_ORIGINAL_word_loc_reconstruct_loss + mean_ORIGINAL_word_touch_reconstruct_loss, timestep)\n","      writer_word.add_scalar('Z_LOSS/mean_ORIGINAL_termination_loss', mean_ORIGINAL_word_termination_loss, timestep)\n","      writer_word.add_scalar('Loss_Loc/mean_ORIGINAL_loc_reconstruct_loss', mean_ORIGINAL_word_loc_reconstruct_loss, timestep)\n","      writer_word.add_scalar('Z_LOSS/mean_ORIGINAL_touch_reconstruct_loss', mean_ORIGINAL_word_touch_reconstruct_loss, timestep)\n","    if TYPE_A:\n","      writer_word.add_scalar('Loss/mean_TYPE_A_loss', mean_TYPE_A_word_termination_loss + mean_TYPE_A_word_loc_reconstruct_loss + mean_TYPE_A_word_touch_reconstruct_loss, timestep)\n","      writer_word.add_scalar('Z_LOSS/mean_TYPE_A_termination_loss', mean_TYPE_A_word_termination_loss, timestep)\n","      writer_word.add_scalar('Loss_Loc/mean_TYPE_A_loc_reconstruct_loss', mean_TYPE_A_word_loc_reconstruct_loss, timestep)\n","      writer_word.add_scalar('Z_LOSS/mean_TYPE_A_touch_reconstruct_loss', mean_TYPE_A_word_touch_reconstruct_loss, timestep)\n","      writer_word.add_scalar('Z_LOSS/mean_TYPE_A_WC_reconstruct_loss', mean_TYPE_A_word_WC_reconstruct_loss, timestep)\n","    if TYPE_B:\n","      writer_word.add_scalar('Loss/mean_TYPE_B_loss', mean_TYPE_B_word_termination_loss + mean_TYPE_B_word_loc_reconstruct_loss + mean_TYPE_B_word_touch_reconstruct_loss, timestep)\n","      writer_word.add_scalar('Z_LOSS/mean_TYPE_B_termination_loss', mean_TYPE_B_word_termination_loss, timestep)\n","      writer_word.add_scalar('Loss_Loc/mean_TYPE_B_loc_reconstruct_loss', mean_TYPE_B_word_loc_reconstruct_loss, timestep)\n","      writer_word.add_scalar('Z_LOSS/mean_TYPE_B_touch_reconstruct_loss', mean_TYPE_B_word_touch_reconstruct_loss, timestep)\n","      writer_word.add_scalar('Z_LOSS/mean_TYPE_B_WC_reconstruct_loss', mean_TYPE_B_word_WC_reconstruct_loss, timestep)\n","    if TYPE_C:\n","      writer_word.add_scalar('Loss/mean_TYPE_C_loss', mean_TYPE_C_word_termination_loss + mean_TYPE_C_word_loc_reconstruct_loss + mean_TYPE_C_word_touch_reconstruct_loss, timestep)\n","      writer_word.add_scalar('Z_LOSS/mean_TYPE_C_termination_loss', mean_TYPE_C_word_termination_loss, timestep)\n","      writer_word.add_scalar('Loss_Loc/mean_TYPE_C_loc_reconstruct_loss', mean_TYPE_C_word_loc_reconstruct_loss, timestep)\n","      writer_word.add_scalar('Z_LOSS/mean_TYPE_C_touch_reconstruct_loss', mean_TYPE_C_word_touch_reconstruct_loss, timestep)\n","      writer_word.add_scalar('Z_LOSS/mean_TYPE_C_WC_reconstruct_loss', mean_TYPE_C_word_WC_reconstruct_loss, timestep)\n","    if TYPE_D:\n","      writer_word.add_scalar('Loss/mean_TYPE_D_loss', mean_TYPE_D_word_termination_loss + mean_TYPE_D_word_loc_reconstruct_loss + mean_TYPE_D_word_touch_reconstruct_loss, timestep)\n","      writer_word.add_scalar('Z_LOSS/mean_TYPE_D_termination_loss', mean_TYPE_D_word_termination_loss, timestep)\n","      writer_word.add_scalar('Loss_Loc/mean_TYPE_D_loc_reconstruct_loss', mean_TYPE_D_word_loc_reconstruct_loss, timestep)\n","      writer_word.add_scalar('Z_LOSS/mean_TYPE_D_touch_reconstruct_loss', mean_TYPE_D_word_touch_reconstruct_loss, timestep)\n","      writer_word.add_scalar('Z_LOSS/mean_TYPE_D_WC_reconstruct_loss', mean_TYPE_D_word_WC_reconstruct_loss, timestep)\n","\n","  if segment_loss:\n","    [total_segment_loss, mean_segment_W_consistency_loss, mean_ORIGINAL_segment_termination_loss, mean_ORIGINAL_segment_loc_reconstruct_loss, mean_ORIGINAL_segment_touch_reconstruct_loss, mean_TYPE_A_segment_termination_loss, mean_TYPE_A_segment_loc_reconstruct_loss, mean_TYPE_A_segment_touch_reconstruct_loss, mean_TYPE_B_segment_termination_loss, mean_TYPE_B_segment_loc_reconstruct_loss, mean_TYPE_B_segment_touch_reconstruct_loss, mean_TYPE_A_segment_WC_reconstruct_loss, mean_TYPE_B_segment_WC_reconstruct_loss] = segment_losses\n","    writer_all.add_scalar('ALL/total_segment_loss', total_segment_loss, timestep)\n","    writer_segment.add_scalar('Loss/mean_W_consistency_loss', mean_segment_W_consistency_loss, timestep)\n","    if ORIGINAL:\n","      writer_segment.add_scalar('Loss/mean_ORIGINAL_loss', mean_ORIGINAL_segment_termination_loss + mean_ORIGINAL_segment_loc_reconstruct_loss + mean_ORIGINAL_segment_touch_reconstruct_loss, timestep)\n","      writer_segment.add_scalar('Z_LOSS/mean_ORIGINAL_termination_loss', mean_ORIGINAL_segment_termination_loss, timestep)\n","      writer_segment.add_scalar('Loss_Loc/mean_ORIGINAL_loc_reconstruct_loss', mean_ORIGINAL_segment_loc_reconstruct_loss, timestep)\n","      writer_segment.add_scalar('Z_LOSS/mean_ORIGINAL_touch_reconstruct_loss', mean_ORIGINAL_segment_touch_reconstruct_loss, timestep)\n","    if TYPE_A:\n","      writer_segment.add_scalar('Loss/mean_TYPE_A_loss', mean_TYPE_A_segment_termination_loss + mean_TYPE_A_segment_loc_reconstruct_loss + mean_TYPE_A_segment_touch_reconstruct_loss, timestep)\n","      writer_segment.add_scalar('Z_LOSS/mean_TYPE_A_termination_loss', mean_TYPE_A_segment_termination_loss, timestep)\n","      writer_segment.add_scalar('Loss_Loc/mean_TYPE_A_loc_reconstruct_loss', mean_TYPE_A_segment_loc_reconstruct_loss, timestep)\n","      writer_segment.add_scalar('Z_LOSS/mean_TYPE_A_touch_reconstruct_loss', mean_TYPE_A_segment_touch_reconstruct_loss, timestep)\n","      writer_segment.add_scalar('Z_LOSS/mean_TYPE_A_WC_reconstruct_loss', mean_TYPE_A_segment_WC_reconstruct_loss, timestep)\n","    if TYPE_B:\n","      writer_segment.add_scalar('Loss/mean_TYPE_B_loss', mean_TYPE_B_segment_termination_loss + mean_TYPE_B_segment_loc_reconstruct_loss + mean_TYPE_B_segment_touch_reconstruct_loss, timestep)\n","      writer_segment.add_scalar('Z_LOSS/mean_TYPE_B_termination_loss', mean_TYPE_B_segment_termination_loss, timestep)\n","      writer_segment.add_scalar('Loss_Loc/mean_TYPE_B_loc_reconstruct_loss', mean_TYPE_B_segment_loc_reconstruct_loss, timestep)\n","      writer_segment.add_scalar('Z_LOSS/mean_TYPE_B_touch_reconstruct_loss', mean_TYPE_B_segment_touch_reconstruct_loss, timestep)\n","      writer_segment.add_scalar('Z_LOSS/mean_TYPE_B_WC_reconstruct_loss', mean_TYPE_B_segment_WC_reconstruct_loss, timestep)\n","\n","  total_loss.backward()\n","\n","  torch.nn.utils.clip_grad_norm_(net.parameters(), grad_clip)\n","  for p in net.parameters():\n","    if p.grad is not None:\n","      # p.data.add_(-lr, p.grad.data)\n","      p.data.add_(p.grad.data, alpha=-lr)\n","\n","  optimizer.step()\n","\n","  if timestep % (num_writer * num_samples * 1) == 0.0:\n","    commands_list = net.sample([\tbatch_word_level_stroke_in, batch_word_level_stroke_out, batch_word_level_stroke_length,\n","                    batch_word_level_term, batch_word_level_char, batch_word_level_char_length, batch_segment_level_stroke_in,\n","                    batch_segment_level_stroke_out, batch_segment_level_stroke_length, batch_segment_level_term,\n","                    batch_segment_level_char, batch_segment_level_char_length])\n","    [t_commands, o_commands, a_commands, b_commands, c_commands, d_commands] = commands_list\n","\n","    t_im = Image.fromarray(np.zeros([160, 750]))\n","    t_dr = ImageDraw.Draw(t_im)\n","\n","    px, py = 30, 100\n","    for i, [dx,dy,t] in enumerate(t_commands):\n","      x = px + dx * 5\n","      y = py + dy * 5\n","      if t == 0:\n","        t_dr.line((px,py,x,y),255,1)\n","      px, py = x, y\n","\n","    o_im = Image.fromarray(np.zeros([160, 750]))\n","    o_dr = ImageDraw.Draw(o_im)\n","    px, py = 30, 100\n","    for i, [dx,dy,t] in enumerate(o_commands):\n","      x = px + dx * 5\n","      y = py + dy * 5\n","      if t == 0:\n","        o_dr.line((px,py,x,y),255,1)\n","      px, py = x, y\n","\n","    a_im = Image.fromarray(np.zeros([160, 750]))\n","    a_dr = ImageDraw.Draw(a_im)\n","    px, py = 30, 100\n","    for i, [dx,dy,t] in enumerate(a_commands):\n","      x = px + dx * 5\n","      y = py + dy * 5\n","      if t == 0:\n","        a_dr.line((px,py,x,y),255,1)\n","      px, py = x, y\n","\n","    b_im = Image.fromarray(np.zeros([160, 750]))\n","    b_dr = ImageDraw.Draw(b_im)\n","    px, py = 30, 100\n","    for i, [dx,dy,t] in enumerate(b_commands):\n","      x = px + dx * 5\n","      y = py + dy * 5\n","      if t == 0:\n","        b_dr.line((px,py,x,y),255,1)\n","      px, py = x, y\n","\n","    c_im = Image.fromarray(np.zeros([160, 750]))\n","    c_dr = ImageDraw.Draw(c_im)\n","    px, py = 30, 100\n","    for i, [dx,dy,t] in enumerate(c_commands):\n","      x = px + dx * 5\n","      y = py + dy * 5\n","      if t == 0:\n","        c_dr.line((px,py,x,y),255,1)\n","      px, py = x, y\n","\n","    d_im = Image.fromarray(np.zeros([160, 750]))\n","    d_dr = ImageDraw.Draw(d_im)\n","    px, py = 30, 100\n","    for i, [dx,dy,t] in enumerate(d_commands):\n","      x = px + dx * 5\n","      y = py + dy * 5\n","      if t == 0:\n","        d_dr.line((px,py,x,y),255,1)\n","      px, py = x, y\n","\n","    dst = Image.new('RGB', (750, 960))\n","    dst.paste(t_im, (0, 0))\n","    dst.paste(o_im, (0, 160))\n","    dst.paste(a_im, (0, 320))\n","    dst.paste(b_im, (0, 480))\n","    dst.paste(c_im, (0, 640))\n","    dst.paste(d_im, (0, 800))\n","    writer_all.add_image('Res/Results', np.asarray(dst.convert(\"RGB\")), timestep, dataformats='HWC')\n","\n","  if VALIDATION:\n","    [all_sentence_level_stroke_in, all_sentence_level_stroke_out, all_sentence_level_stroke_length, all_sentence_level_term,\n","    all_sentence_level_char, all_sentence_level_char_length, all_word_level_stroke_in, all_word_level_stroke_out,\n","    all_word_level_stroke_length, all_word_level_term, all_word_level_char, all_word_level_char_length,\n","    all_segment_level_stroke_in, all_segment_level_stroke_out, all_segment_level_stroke_length, all_segment_level_term,\n","    all_segment_level_char, all_segment_level_char_length] = dl.next_batch(TYPE='VALID')\n","\n","    batch_sentence_level_stroke_in \t\t= [torch.FloatTensor(a).to(device) for a in all_sentence_level_stroke_in]\n","    batch_sentence_level_stroke_out \t= [torch.FloatTensor(a).to(device) for a in all_sentence_level_stroke_out]\n","    batch_sentence_level_stroke_length \t= [torch.LongTensor(a).to(device).unsqueeze(-1) for a in all_sentence_level_stroke_length]\n","    batch_sentence_level_term \t\t\t= [torch.FloatTensor(a).to(device) for a in all_sentence_level_term]\n","    batch_sentence_level_char \t\t\t= [torch.LongTensor(a).to(device) for a in all_sentence_level_char]\n","    batch_sentence_level_char_length \t= [torch.LongTensor(a).to(device).unsqueeze(-1) for a in all_sentence_level_char_length]\n","    batch_word_level_stroke_in \t\t\t= [torch.FloatTensor(a).to(device) for a in all_word_level_stroke_in]\n","    batch_word_level_stroke_out \t\t= [torch.FloatTensor(a).to(device) for a in all_word_level_stroke_out]\n","    batch_word_level_stroke_length \t\t= [torch.LongTensor(a).to(device).unsqueeze(-1) for a in all_word_level_stroke_length]\n","    batch_word_level_term \t\t\t\t= [torch.FloatTensor(a).to(device) for a in all_word_level_term]\n","    batch_word_level_char \t\t\t\t= [torch.LongTensor(a).to(device) for a in all_word_level_char]\n","    batch_word_level_char_length \t\t= [torch.LongTensor(a).to(device).unsqueeze(-1) for a in all_word_level_char_length]\n","    batch_segment_level_stroke_in \t\t= [[torch.FloatTensor(a).to(device) for a in b] for b in all_segment_level_stroke_in]\n","    batch_segment_level_stroke_out \t\t= [[torch.FloatTensor(a).to(device) for a in b] for b in all_segment_level_stroke_out]\n","    batch_segment_level_stroke_length \t= [[torch.LongTensor(a).to(device).unsqueeze(-1) for a in b] for b in all_segment_level_stroke_length]\n","    batch_segment_level_term \t\t\t= [[torch.FloatTensor(a).to(device) for a in b] for b in all_segment_level_term]\n","    batch_segment_level_char \t\t\t= [[torch.LongTensor(a).to(device) for a in b] for b in all_segment_level_char]\n","    batch_segment_level_char_length \t= [[torch.LongTensor(a).to(device).unsqueeze(-1) for a in b] for b in all_segment_level_char_length]\n","\n","    res = net([batch_sentence_level_stroke_in, batch_sentence_level_stroke_out, batch_sentence_level_stroke_length,\n","        batch_sentence_level_term, batch_sentence_level_char, batch_sentence_level_char_length,\n","        batch_word_level_stroke_in, batch_word_level_stroke_out, batch_word_level_stroke_length,\n","        batch_word_level_term, batch_word_level_char, batch_word_level_char_length, batch_segment_level_stroke_in,\n","        batch_segment_level_stroke_out, batch_segment_level_stroke_length, batch_segment_level_term,\n","        batch_segment_level_char, batch_segment_level_char_length])\n","\n","    total_loss, sentence_losses, word_losses, segment_losses = res\n","\n","    valid_writer_all.add_scalar('ALL/total_loss', total_loss, timestep)\n","\n","    if sentence_loss:\n","      [total_sentence_loss, mean_sentence_W_consistency_loss, mean_ORIGINAL_sentence_termination_loss, mean_ORIGINAL_sentence_loc_reconstruct_loss, mean_ORIGINAL_sentence_touch_reconstruct_loss, mean_TYPE_A_sentence_termination_loss, mean_TYPE_A_sentence_loc_reconstruct_loss, mean_TYPE_A_sentence_touch_reconstruct_loss, mean_TYPE_B_sentence_termination_loss, mean_TYPE_B_sentence_loc_reconstruct_loss, mean_TYPE_B_sentence_touch_reconstruct_loss, mean_TYPE_A_sentence_WC_reconstruct_loss, mean_TYPE_B_sentence_WC_reconstruct_loss] = sentence_losses\n","\n","      valid_writer_all.add_scalar('ALL/total_sentence_loss', total_sentence_loss, timestep)\n","      valid_writer_sentence.add_scalar('Loss/mean_W_consistency_loss', mean_sentence_W_consistency_loss, timestep)\n","      if ORIGINAL:\n","        valid_writer_sentence.add_scalar('Loss/mean_ORIGINAL_loss', mean_ORIGINAL_sentence_termination_loss + mean_ORIGINAL_sentence_loc_reconstruct_loss + mean_ORIGINAL_sentence_touch_reconstruct_loss, timestep)\n","        valid_writer_sentence.add_scalar('Z_LOSS/mean_ORIGINAL_termination_loss', mean_ORIGINAL_sentence_termination_loss, timestep)\n","        valid_writer_sentence.add_scalar('Loss_Loc/mean_ORIGINAL_loc_reconstruct_loss', mean_ORIGINAL_sentence_loc_reconstruct_loss, timestep)\n","        valid_writer_sentence.add_scalar('Z_LOSS/mean_ORIGINAL_touch_reconstruct_loss', mean_ORIGINAL_sentence_touch_reconstruct_loss, timestep)\n","      if TYPE_A:\n","        valid_writer_sentence.add_scalar('Loss/mean_TYPE_A_loss', mean_TYPE_A_sentence_termination_loss + mean_TYPE_A_sentence_loc_reconstruct_loss + mean_TYPE_A_sentence_touch_reconstruct_loss, timestep)\n","        valid_writer_sentence.add_scalar('Z_LOSS/mean_TYPE_A_termination_loss', mean_TYPE_A_sentence_termination_loss, timestep)\n","        valid_writer_sentence.add_scalar('Loss_Loc/mean_TYPE_A_loc_reconstruct_loss', mean_TYPE_A_sentence_loc_reconstruct_loss, timestep)\n","        valid_writer_sentence.add_scalar('Z_LOSS/mean_TYPE_A_touch_reconstruct_loss', mean_TYPE_A_sentence_touch_reconstruct_loss, timestep)\n","        valid_writer_sentence.add_scalar('Z_LOSS/mean_TYPE_A_WC_reconstruct_loss', mean_TYPE_A_sentence_WC_reconstruct_loss, timestep)\n","      if TYPE_B:\n","        valid_writer_sentence.add_scalar('Loss/mean_TYPE_B_loss', mean_TYPE_B_sentence_termination_loss + mean_TYPE_B_sentence_loc_reconstruct_loss + mean_TYPE_B_sentence_touch_reconstruct_loss, timestep)\n","        valid_writer_sentence.add_scalar('Z_LOSS/mean_TYPE_B_termination_loss', mean_TYPE_B_sentence_termination_loss, timestep)\n","        valid_writer_sentence.add_scalar('Loss_Loc/mean_TYPE_B_loc_reconstruct_loss', mean_TYPE_B_sentence_loc_reconstruct_loss, timestep)\n","        valid_writer_sentence.add_scalar('Z_LOSS/mean_TYPE_B_touch_reconstruct_loss', mean_TYPE_B_sentence_touch_reconstruct_loss, timestep)\n","        valid_writer_sentence.add_scalar('Z_LOSS/mean_TYPE_B_WC_reconstruct_loss', mean_TYPE_B_sentence_WC_reconstruct_loss, timestep)\n","\n","    if word_loss:\n","      [total_word_loss, mean_word_W_consistency_loss, mean_ORIGINAL_word_termination_loss, mean_ORIGINAL_word_loc_reconstruct_loss, mean_ORIGINAL_word_touch_reconstruct_loss, mean_TYPE_A_word_termination_loss, mean_TYPE_A_word_loc_reconstruct_loss, mean_TYPE_A_word_touch_reconstruct_loss, mean_TYPE_B_word_termination_loss, mean_TYPE_B_word_loc_reconstruct_loss, mean_TYPE_B_word_touch_reconstruct_loss, mean_TYPE_C_word_termination_loss, mean_TYPE_C_word_loc_reconstruct_loss, mean_TYPE_C_word_touch_reconstruct_loss, mean_TYPE_D_word_termination_loss, mean_TYPE_D_word_loc_reconstruct_loss, mean_TYPE_D_word_touch_reconstruct_loss, mean_TYPE_A_word_WC_reconstruct_loss, mean_TYPE_B_word_WC_reconstruct_loss, mean_TYPE_C_word_WC_reconstruct_loss, mean_TYPE_D_word_WC_reconstruct_loss] = word_losses\n","      valid_writer_all.add_scalar('ALL/total_word_loss', total_word_loss, timestep)\n","      valid_writer_word.add_scalar('Loss/mean_W_consistency_loss', mean_word_W_consistency_loss, timestep)\n","\n","      if ORIGINAL:\n","        valid_writer_word.add_scalar('Loss/mean_ORIGINAL_loss', mean_ORIGINAL_word_termination_loss + mean_ORIGINAL_word_loc_reconstruct_loss + mean_ORIGINAL_word_touch_reconstruct_loss, timestep)\n","        valid_writer_word.add_scalar('Z_LOSS/mean_ORIGINAL_termination_loss', mean_ORIGINAL_word_termination_loss, timestep)\n","        valid_writer_word.add_scalar('Loss_Loc/mean_ORIGINAL_loc_reconstruct_loss', mean_ORIGINAL_word_loc_reconstruct_loss, timestep)\n","        valid_writer_word.add_scalar('Z_LOSS/mean_ORIGINAL_touch_reconstruct_loss', mean_ORIGINAL_word_touch_reconstruct_loss, timestep)\n","      if TYPE_A:\n","        valid_writer_word.add_scalar('Loss/mean_TYPE_A_loss', mean_TYPE_A_word_termination_loss + mean_TYPE_A_word_loc_reconstruct_loss + mean_TYPE_A_word_touch_reconstruct_loss, timestep)\n","        valid_writer_word.add_scalar('Z_LOSS/mean_TYPE_A_termination_loss', mean_TYPE_A_word_termination_loss, timestep)\n","        valid_writer_word.add_scalar('Loss_Loc/mean_TYPE_A_loc_reconstruct_loss', mean_TYPE_A_word_loc_reconstruct_loss, timestep)\n","        valid_writer_word.add_scalar('Z_LOSS/mean_TYPE_A_touch_reconstruct_loss', mean_TYPE_A_word_touch_reconstruct_loss, timestep)\n","        valid_writer_word.add_scalar('Z_LOSS/mean_TYPE_A_WC_reconstruct_loss', mean_TYPE_A_word_WC_reconstruct_loss, timestep)\n","      if TYPE_B:\n","        valid_writer_word.add_scalar('Loss/mean_TYPE_B_loss', mean_TYPE_B_word_termination_loss + mean_TYPE_B_word_loc_reconstruct_loss + mean_TYPE_B_word_touch_reconstruct_loss, timestep)\n","        valid_writer_word.add_scalar('Z_LOSS/mean_TYPE_B_termination_loss', mean_TYPE_B_word_termination_loss, timestep)\n","        valid_writer_word.add_scalar('Loss_Loc/mean_TYPE_B_loc_reconstruct_loss', mean_TYPE_B_word_loc_reconstruct_loss, timestep)\n","        valid_writer_word.add_scalar('Z_LOSS/mean_TYPE_B_touch_reconstruct_loss', mean_TYPE_B_word_touch_reconstruct_loss, timestep)\n","        valid_writer_word.add_scalar('Z_LOSS/mean_TYPE_B_WC_reconstruct_loss', mean_TYPE_B_word_WC_reconstruct_loss, timestep)\n","      if TYPE_C:\n","        valid_writer_word.add_scalar('Loss/mean_TYPE_C_loss', mean_TYPE_C_word_termination_loss + mean_TYPE_C_word_loc_reconstruct_loss + mean_TYPE_C_word_touch_reconstruct_loss, timestep)\n","        valid_writer_word.add_scalar('Z_LOSS/mean_TYPE_C_termination_loss', mean_TYPE_C_word_termination_loss, timestep)\n","        valid_writer_word.add_scalar('Loss_Loc/mean_TYPE_C_loc_reconstruct_loss', mean_TYPE_C_word_loc_reconstruct_loss, timestep)\n","        valid_writer_word.add_scalar('Z_LOSS/mean_TYPE_C_touch_reconstruct_loss', mean_TYPE_C_word_touch_reconstruct_loss, timestep)\n","        valid_writer_word.add_scalar('Z_LOSS/mean_TYPE_C_WC_reconstruct_loss', mean_TYPE_C_word_WC_reconstruct_loss, timestep)\n","      if TYPE_D:\n","        valid_writer_word.add_scalar('Loss/mean_TYPE_D_loss', mean_TYPE_D_word_termination_loss + mean_TYPE_D_word_loc_reconstruct_loss + mean_TYPE_D_word_touch_reconstruct_loss, timestep)\n","        valid_writer_word.add_scalar('Z_LOSS/mean_TYPE_D_termination_loss', mean_TYPE_D_word_termination_loss, timestep)\n","        valid_writer_word.add_scalar('Loss_Loc/mean_TYPE_D_loc_reconstruct_loss', mean_TYPE_D_word_loc_reconstruct_loss, timestep)\n","        valid_writer_word.add_scalar('Z_LOSS/mean_TYPE_D_touch_reconstruct_loss', mean_TYPE_D_word_touch_reconstruct_loss, timestep)\n","        valid_writer_word.add_scalar('Z_LOSS/mean_TYPE_D_WC_reconstruct_loss', mean_TYPE_D_word_WC_reconstruct_loss, timestep)\n","\n","    if segment_loss:\n","      [total_segment_loss, mean_segment_W_consistency_loss, mean_ORIGINAL_segment_termination_loss, mean_ORIGINAL_segment_loc_reconstruct_loss, mean_ORIGINAL_segment_touch_reconstruct_loss, mean_TYPE_A_segment_termination_loss, mean_TYPE_A_segment_loc_reconstruct_loss, mean_TYPE_A_segment_touch_reconstruct_loss, mean_TYPE_B_segment_termination_loss, mean_TYPE_B_segment_loc_reconstruct_loss, mean_TYPE_B_segment_touch_reconstruct_loss, mean_TYPE_A_segment_WC_reconstruct_loss, mean_TYPE_B_segment_WC_reconstruct_loss] = segment_losses\n","      valid_writer_all.add_scalar('ALL/total_segment_loss', total_segment_loss, timestep)\n","      valid_writer_segment.add_scalar('Loss/mean_W_consistency_loss', mean_segment_W_consistency_loss, timestep)\n","      if ORIGINAL:\n","        valid_writer_segment.add_scalar('Loss/mean_ORIGINAL_loss', mean_ORIGINAL_segment_termination_loss + mean_ORIGINAL_segment_loc_reconstruct_loss + mean_ORIGINAL_segment_touch_reconstruct_loss, timestep)\n","        valid_writer_segment.add_scalar('Z_LOSS/mean_ORIGINAL_termination_loss', mean_ORIGINAL_segment_termination_loss, timestep)\n","        valid_writer_segment.add_scalar('Loss_Loc/mean_ORIGINAL_loc_reconstruct_loss', mean_ORIGINAL_segment_loc_reconstruct_loss, timestep)\n","        valid_writer_segment.add_scalar('Z_LOSS/mean_ORIGINAL_touch_reconstruct_loss', mean_ORIGINAL_segment_touch_reconstruct_loss, timestep)\n","      if TYPE_A:\n","        valid_writer_segment.add_scalar('Loss/mean_TYPE_A_loss', mean_TYPE_A_segment_termination_loss + mean_TYPE_A_segment_loc_reconstruct_loss + mean_TYPE_A_segment_touch_reconstruct_loss, timestep)\n","        valid_writer_segment.add_scalar('Z_LOSS/mean_TYPE_A_termination_loss', mean_TYPE_A_segment_termination_loss, timestep)\n","        valid_writer_segment.add_scalar('Loss_Loc/mean_TYPE_A_loc_reconstruct_loss', mean_TYPE_A_segment_loc_reconstruct_loss, timestep)\n","        valid_writer_segment.add_scalar('Z_LOSS/mean_TYPE_A_touch_reconstruct_loss', mean_TYPE_A_segment_touch_reconstruct_loss, timestep)\n","        valid_writer_segment.add_scalar('Z_LOSS/mean_TYPE_A_WC_reconstruct_loss', mean_TYPE_A_segment_WC_reconstruct_loss, timestep)\n","      if TYPE_B:\n","        valid_writer_segment.add_scalar('Loss/mean_TYPE_B_loss', mean_TYPE_B_segment_termination_loss + mean_TYPE_B_segment_loc_reconstruct_loss + mean_TYPE_B_segment_touch_reconstruct_loss, timestep)\n","        valid_writer_segment.add_scalar('Z_LOSS/mean_TYPE_B_termination_loss', mean_TYPE_B_segment_termination_loss, timestep)\n","        valid_writer_segment.add_scalar('Loss_Loc/mean_TYPE_B_loc_reconstruct_loss', mean_TYPE_B_segment_loc_reconstruct_loss, timestep)\n","        valid_writer_segment.add_scalar('Z_LOSS/mean_TYPE_B_touch_reconstruct_loss', mean_TYPE_B_segment_touch_reconstruct_loss, timestep)\n","        valid_writer_segment.add_scalar('Z_LOSS/mean_TYPE_B_WC_reconstruct_loss', mean_TYPE_B_segment_WC_reconstruct_loss, timestep)\n","\n","  if timestep % 100 < 10: #(num_writer * num_samples * 100) == 0.0:\n","    torch.save({'timestep': timestep,\n","            'model_state_dict': net.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': total_loss.item(),\n","            }, 'model/'+str(timestep)+'.pt')\n","\n","writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zik1scGdJo5l"},"outputs":[],"source":["# run this to view the loss graphs\n","%load_ext tensorboard\n","%tensorboard --logdir runs"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"run.ipynb","provenance":[],"toc_visible":true,"mount_file_id":"184nWntMbVMZIatSca3oBj5823vKw1sWJ","authorship_tag":"ABX9TyNCni/pGOBLzpmpHshCEFjs"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}